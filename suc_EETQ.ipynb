{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yx_HJkRHaZC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mT-PO-F6cpry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wnPIuJrecpug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline('text-generation', model=\"facebook/opt-125m\")\n",
        "generator(\"What are we having for dinner?\")"
      ],
      "metadata": {
        "id": "g_jpV_5bcpxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ", torch_dtype=torch.float32to\""
      ],
      "metadata": {
        "id": "Hp_1b8YvcqRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "generator = pipeline('text-generation', model=\"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\", torch_dtype=torch.float16, device_map=\"auto\")\n",
        "generator(\"What are we having for dinner?\")"
      ],
      "metadata": {
        "id": "BXHhMR8vc8Sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "pipe = pipeline(\"text-generation\", model=\"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\", trust_remote_code=True, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "pipe(messages)"
      ],
      "metadata": {
        "id": "4dneb3IPfuQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Cornell-RelaxML/QuIP.git"
      ],
      "metadata": {
        "id": "FdqJvbVNc-e5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/QuIP\n",
        "!CUDA_VISIBLE_DEVICES=0 python opt.py facebook/opt-125m c4"
      ],
      "metadata": {
        "id": "asyPNF4Dd_UC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install primefac==1.1"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Yxdguw7kgrBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/QuIP\n",
        "!CUDA_VISIBLE_DEVICES=0 python opt.py facebook/opt-125m"
      ],
      "metadata": {
        "id": "raGoZnoOg_pE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/a.txt"
      ],
      "metadata": {
        "id": "LokQq23nhZ3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes peft trl"
      ],
      "metadata": {
        "id": "CFTSb4jsioPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/IST-DASLab/gptq.git"
      ],
      "metadata": {
        "id": "EPOMpf9Gjf-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gptq\n",
        "!CUDA_VISIBLE_DEVICES=0 python test_kernel.py"
      ],
      "metadata": {
        "id": "Y01KLooijvtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python setup_cuda.py install"
      ],
      "metadata": {
        "id": "DfADvZ4Ej6Xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python opt.py facebook/opt-125m c4"
      ],
      "metadata": {
        "id": "0sMctJbfjVnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import datasets\n",
        "datasets.load_dataset('c4', 'en') # This might take some time because it downloads a lot of data"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "vL_UJ-nTlvTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python /content/gptq/opt.py facebook/opt-125m c4"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "_MnqAIIGlyKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/IST-DASLab/gptq"
      ],
      "metadata": {
        "id": "RLKfAScfjPA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/qwopqwop200/GPTQ-for-LLaMa"
      ],
      "metadata": {
        "id": "Rgd1JwlUoZ-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UkOxBfmCoZ83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM\n",
        "import torch\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_storage=torch.bfloat16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"facebook/opt-125m\",\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")"
      ],
      "metadata": {
        "id": "QJ4HEJeThcmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال"
      ],
      "metadata": {
        "id": "PA3fEBSxperH"
      }
    },
    {
      "source": [
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Configure BitsAndBytes for 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_storage=torch.bfloat16,\n",
        ")\n",
        "\n",
        "# Load the quantized model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"facebook/opt-125m\",\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
        "\n",
        "# Prepare input text\n",
        "text = \"What are we having for dinner?\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Generate text\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
        "generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "# Print the generated text\n",
        "print(generated_text)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "R45Pa_0hpK1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hd6Bt7D6pdrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XaSnIihgqTke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KZLb92u5qThu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nIGVp2DFqTeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DAdKa-26qTcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pFv3_cF0qTYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fwn2l_RyqTVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EETQ\n",
        "تدعم مكتبة EETQ تكميم الوزن فقط لكل قناة int8 لوحدات معالجة الرسومات NVIDIA. نواة GEMM و GEMV عالية الأداء من FasterTransformer و TensorRT-LLM. لا يتطلب أي مجموعة بيانات معايرة ولا يحتاج إلى تحديد النموذج الخاص بك مسبقا. وعلاوة على ذلك، فإن تدهور الدقة لا يكاد يذكر بسبب التكميم لكل قناة."
      ],
      "metadata": {
        "id": "O_txFkhgqXZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-cache-dir https://github.com/NetEase-FuXi/EETQ/releases/download/v1.0.0/EETQ-1.0.0+cu121+torch2.1.2-cp310-cp310-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "pX7QjlfVqTSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/NetEase-FuXi/EETQ/releases/download/v1.0.1/EETQ-1.0.1-cp311-cp311-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "kLapmlYhqVMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install EETQ-1.0.1-cp311-cp311-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "WW0IQAk2qnQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, EetqConfig\n",
        "path = \"facebook/opt-125m\"\n",
        "quantization_config = EetqConfig(\"int8\")\n",
        "model = AutoModelForCausalLM.from_pretrained(path, device_map=\"auto\", quantization_config=quantization_config)"
      ],
      "metadata": {
        "id": "pncez4Wqqr12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i-LDBH2Zq0K2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install --no-cache-dir https://github.com/NetEase-FuXi/EETQ/releases/download/v1.0.0/EETQ-1.0.0+cu121+torch2.1.2-cp310-cp310-linux_x86_64.whl\n",
        "!wget https://github.com/NetEase-FuXi/EETQ/releases/download/v1.0.1/EETQ-1.0.1-cp311-cp311-linux_x86_64.whl\n",
        "!pip install EETQ-1.0.1-cp311-cp311-linux_x86_64.whl"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "_-eiM2bZrBCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip uninstall -y EETQ # Uninstall any existing EETQ installations to avoid conflicts\n",
        "!pip install eetq # Install the latest EETQ version that's compatible with your environment\n",
        "\n",
        "from transformers import AutoModelForCausalLM, EetqConfig\n",
        "\n",
        "path = \"facebook/opt-125m\"\n",
        "quantization_config = EetqConfig(\"int8\")\n",
        "model = AutoModelForCausalLM.from_pretrained(path, device_map=\"auto\", quantization_config=quantization_config)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "TBBy8zqwrGBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NetEase-FuXi/EETQ.git\n",
        "%cd EETQ/\n",
        "!git submodule update --init --recursive\n",
        "!pip install .\n"
      ],
      "metadata": {
        "id": "1gQBLp1JrRLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/EETQ\n",
        "!python setup.py setup"
      ],
      "metadata": {
        "id": "WrT_9xy4rojw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install e ."
      ],
      "metadata": {
        "id": "FR85cGmbruzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/EETQ/python/eetq/models/llama.py"
      ],
      "metadata": {
        "id": "a615HCa2sJ5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install eetq"
      ],
      "metadata": {
        "id": "klgLY__YsUxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, EetqConfig\n",
        "path = \"facebook/opt-125m\"\n",
        "quantization_config = EetqConfig(\"int8\")\n",
        "model = AutoModelForCausalLM.from_pretrained(path, device_map=\"auto\", quantization_config=quantization_config)"
      ],
      "metadata": {
        "id": "7LDg-8SSsZln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from eetq.utils import eet_quantize\n",
        "path = \"facebook/opt-125m\"\n",
        "eet_quantize(torch_model)"
      ],
      "metadata": {
        "id": "j8XuoHoosnf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/NetEase-FuXi/EETQ/issues/35"
      ],
      "metadata": {
        "id": "Kxx0lp9Tta-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ayشغال%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
      ],
      "metadata": {
        "id": "C9ykdy_0ubKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NetEase-FuXi/EETQ.git\n",
        "%cd EETQ/\n",
        "!git submodule update --init --recursive\n",
        "\n",
        "!wget https://github.com/NetEase-FuXi/EETQ/releases/download/v1.0.1/EETQ-1.0.1-cp311-cp311-linux_x86_64.whl\n",
        "!pip install /content/EETQ-1.0.1-cp311-cp311-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "zAsJd9t3tLvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token XXXXXXXX"
      ],
      "metadata": {
        "id": "qAiANmLLtiKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, EetqConfig\n",
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "quantization_config = EetqConfig(\"int8\")\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", quantization_config=quantization_config)\n"
      ],
      "metadata": {
        "id": "hdcxhAwLteTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quant_path = \"/content/a\"\n",
        "model.save_pretrained(quant_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(quant_path, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "PXo71acut3KB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, EetqConfig\n",
        "import torch\n",
        "\n",
        "quantization_config = EetqConfig(\"int8\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"/content/a\", config=quantization_config, torch_dtype=torch.float16)\n"
      ],
      "metadata": {
        "id": "z-_6t3wgt6Xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_name_or_path = \"meta-llama/Llama-2-7b-chat-hf\" # Replace with your original model name/path\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n"
      ],
      "metadata": {
        "id": "2Xk3nwEgt8qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from eetq.utils import eet_accelerator\n",
        "eet_accelerator(model, quantize=False, fused_attn=False, dev=\"cuda:0\")\n",
        "model.to(\"cuda:0\")\n",
        "\n",
        "\n",
        "text = \"Who is Napoleon Bonaparte?\"\n",
        "\n",
        "\n",
        "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(\"cuda:0\")\n",
        "\n",
        "\n",
        "res = model.generate(input_ids, max_length=12)\n",
        "\n",
        "\n",
        "print(tokenizer.decode(res[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "p5dz1eHauBJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$"
      ],
      "metadata": {
        "id": "2MN9YXykxIdS"
      }
    },
    {
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, EetqConfig\n",
        "import torch\n",
        "\n",
        "# Load the model and configuration from the quantized model path\n",
        "quantization_config = EetqConfig(\"int8\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"/content/a\", config=quantization_config, torch_dtype=torch.float16)\n",
        "\n",
        "# Load the tokenizer using the original model name or path\n",
        "model_name_or_path = \"meta-llama/Llama-2-7b-chat-hf\"  # Replace with your original model name/path\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "\n",
        "# Apply EET acceleration\n",
        "from eetq.utils import eet_accelerator\n",
        "\n",
        "# Set `quantize=False` to avoid requiring gradients on quantized tensors\n",
        "eet_accelerator(model, quantize=False, fused_attn=True, dev=\"cuda:0\")\n",
        "model.to(\"cuda:0\")\n",
        "\n",
        "# Prepare your input text\n",
        "text = \"Who is Napoleon Bonaparte?\"\n",
        "\n",
        "# Tokenize the input text\n",
        "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(\"cuda:0\")\n",
        "\n",
        "# Generate text\n",
        "res = model.generate(input_ids, max_length=128)\n",
        "\n",
        "# Decode the generated output\n",
        "print(tokenizer.decode(res[0], skip_special_tokens=True))"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "mCxBDIz0wqc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!huggingface-cli login --token xxxxxxxxxxxxx\n",
        "\n",
        "from transformers import AutoModelForCausalLM, EetqConfig\n",
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "quantization_config = EetqConfig(\"int8\")\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", quantization_config=quantization_config)\n",
        "\n",
        "quant_path = \"/content/a\"\n",
        "model.save_pretrained(quant_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(quant_path, device_map=\"auto\")\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, EetqConfig\n",
        "\n",
        "Load the model and configuration from the quantized model path\n",
        "quantization_config = EetqConfig(\"int8\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"/content/a\", config=quantization_config, torch_dtype=torch.float16)\n",
        "\n",
        "Load the tokenizer using the original model name or path\n",
        "model_name_or_path = \"meta-llama/Llama-2-7b-chat-hf\" # Replace with your original model name/path\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "\n",
        "Apply EET acceleration\n",
        "from eetq.utils import eet_accelerator\n",
        "eet_accelerator(model, quantize=True, fused_attn=True, dev=\"cuda:0\")\n",
        "model.to(\"cuda:0\")\n",
        "\n",
        "Prepare your input text\n",
        "text = \"Who is Napoleon Bonaparte?\"\n",
        "\n",
        "Tokenize the input text\n",
        "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(\"cuda:0\")\n",
        "\n",
        "Generate text\n",
        "res = model.generate(input_ids, max_length=128)\n",
        "\n",
        "Decode the generated output\n",
        "print(tokenizer.decode(res[0], skip_special_tokens=True))\n",
        "\n",
        "low_cpu_mem_usage was None, now default to True since model is quantized.\n",
        "Loading checkpoint shards: 100%\n",
        " 2/2 [00:00<00:00,  2.49it/s]\n",
        "[EET][INFO] attention fusion processiong...: 0it [00:00, ?it/s]\n",
        "[EET][INFO] replace with eet weight quantize only linear...: 100%|██████████| 32/32 [00:00<00:00, 25176.84it/s]\n",
        "Who is Napoleon Bonaparte?\n",
        "\n",
        "Napoleon Bonaparte (1769-1821) was a French military and political leader who rose to prominence during the French Revolution and its associated wars. He was Emperor of the French from 1804 until 1815, when he was defeated in the Napoleonic Wars and exiled to the island of Saint Helena, where he died.\n",
        "\n",
        "Napoleon was born on the island of Corsica and studied at the École Militaire in Paris. He quickly rose through the ranks of the French\n",
        "\n",
        "Activity\n",
        "werruww\n",
        "werruww commented on Dec 17, 2024\n",
        "werruww\n",
        "on Dec 17, 2024\n",
        "Author\n",
        "text = \"Write a Python code to print the letter X 100 times.\"\n",
        "\n",
        "Tokenize the input text\n",
        "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(\"cuda:0\")\n",
        "\n",
        "Generate text\n",
        "res = model.generate(input_ids, max_length=128)\n",
        "\n",
        "Decode the generated output\n",
        "print(tokenizer.decode(res[0], skip_special_tokens=True))\n",
        "\n",
        "Write a Python code to print the letter X 100 times.\n",
        "\n",
        "Here is the solution:\n",
        "\n",
        "print(\"X\")\n",
        "for i in range(1, 101):\n",
        "    print(\"X\")\n",
        "Explanation:\n",
        "The print() function is used to print the letter X. The range() function is used to create a sequence of numbers from 1 to 100. The for loop is used to iterate over the sequence and print the letter X for each number.\n",
        "\n",
        "Alternatively, you can use a list comprehension to print\n",
        "\n"
      ],
      "metadata": {
        "id": "QX62wjKJsv7X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}